A iteração de usuários com sistemas digitais tem crescido exponencialmente com o progresso tecnológico \cite{dean2008mapreduce} e uma quantidade massiva de dados vem sendo produzida nos últimos anos \cite{halevy2009unreasonable}. Cliques em paginas de mídias sociais, interações de usuários em serviços de \textit{streaming}, sensores ou dispositivos \textit{smart}\footnote{Também referidos como dispositivos \textit{Internet of Things} (IoT)}, são exemplos de tecnologias que geram dados a uma alta frequência em grande volume diariamente.

Essa grande quantidade de informação se tornou um dos bens mais valiosos do mundo moderno \cite{economist2017world}. Na ``era do Big Data'' \cite{tarnoff2018big, finger2014data}, companhias como Google\textsuperscript{\tiny\textregistered}, Facebook\textsuperscript{\tiny\textregistered}, Netflix\textsuperscript{\tiny\textregistered} e Amazon\textsuperscript{\tiny\textregistered} tem tirado proveito dessas grandes quantidades de dados através da mineração com objetivo de agregar informações valiosas aos usuários desses sistemas, como recomendações de conteúdo (filmes, músicas, livros.) ou filtrar conteúdos customizados para cada usuário.

Grande parte dos sistemas que era baseado em uso de memoria RAM para o processamento se tornou inviável para essa industria que produz dados na escala de \textit{Petabytes} \cite{dean2008mapreduce}, portanto, foi necessário a concepção de novos ambientes para operarem no armazenamento e processamento desses dados \cite{gama2007learning}. Ferramentas baseadas em computação distribuída se apresentaram como uma alternativa viável devido seu potencial de escalabilidade quase linear.

Dentre essas ferramentas podemos exemplificar o Apache Hadoop\textsuperscript{\tiny\textregistered}, que fornece um ambiente de armazenamento distribuído, segmentando e replicando arquivos para que sejam distribuídos aos nós de um \textit{cluster} de forma automática, o Apache Spark\textsuperscript{\tiny\textregistered}, que provê um ambiente de computação distribuída, seguindo o paradigma MapReduce e o Apache Kafka\textsuperscript{\tiny\textregistered} o qual provê um ambiente para lidar com fluxos de dados, o qual é o motivador desse trabalho.

Fluxos de dados, ou \textit{Data Streams}, são conjuntos de dados gerados em tempo real de forma continua a frequências variáveis, estes atrelados aos pacotes de transmissão. Embora fontes como essa gerem dados sem qualquer supervisão ou filtragem, essa quantidade massiva e complexa de dados tem uma capacidade surpreendente de se extrair informação de grande valor, como discutido em ``\textit{The Unreasonable Effectiveness of Data}'' \cite{halevy2009unreasonable}.


Uma das abordagens para extração de informação que tem sobressaído aliado ao \textit{Big Data} é o Aprendizado de Máquina (AM). O AM é o campo da inteligência artificial que estuda métodos para a extração de padrões dos dados, para que esse conhecimento seja aplicado em tarefas futuras \cite{mitchell1997machine,friedman2001elements}.

Como todo método indutivo, algoritmos tradicionais de AM assumem duas premissas que devem ser satisfeitas para que funcionem corretamente: ($i$) o futuro deve se comportar como o passado e ($ii$) eventos futuros devem ser independentes de eventos passado \cite{vapnik2013nature}. Entretanto, em um fluxo de dados continuo (em inglês, \textit{Data Stream}), a distribuição que gera os dados usualmente está mudando com o tempo e dependências temporais podem ocorrer. Logo, essas premissas podem não se manter e a performance dos modelos induzidos podem perecer com o passar do tempo, causando uma má experiência aos usuários desses sistemas  \cite{gama2007learning, Johansson2014}.

Diversos métodos foram desenvolvidos para lidar em ambientes com mudanças temporais, como detecção de mudança de conceito \cite{klinkenberg2000detecting}, retreino periódico de algoritmo \cite{bifet2007learning} ou novas abordagens de algoritmos, especialmente projetados para esse contexto \cite{zang2014comparative}.

Por exemplo, alguns métodos de detecção de mudança de conceito \cite{gama2010knowledge} são baseados em alarmes que disparam quando a diferença da performance do modelo atual e do melhor modelo até então é maior que um dado limiar. Outros métodos, como a indução periódica de modelos, podem não ser suficientes para resolver esses problemas, já que o espaço de hipótese fixado do algoritmo pode não mais ser apropriado ao problema \cite{rossi2014metastream}.

Meta-Aprendizado (MtA) é uma técnica proeminente para resolver essas limitações através da detecção de mudança de conceito e recomendação de algoritmos para melhorar a predição \cite{Anderson2019,VanRijn2016,Zarmehri2015}. Para usar MtA, meta-dados devem ser construídos \cite{Vanschoren2018} baseado em uma conjunto de características descritivas, nomeados meta-atributos, do problema sob analise \cite{Rivolli2018}. Esses meta-atributos são extraídos e combinados criando um meta-exemplo. Diferentes algoritmos de AM são aplicados ao problema, e suas performances são utilizadas para rotular os meta-exemplos. Esse procedimento é performado para o fluxo de dados em diferentes pontos no tempo, resultando em um conjunto de meta-dados, o qual pode ser usado para induzir um modelo capaz de predizer qual será o algoritmo mais apropriado para eventos futuros sem os altos custos usuais que o processo de seleção de modelos de ML demanda \cite{Munoz2018}.

Nesse trabalho o MetaStream \cite{rossi2012, rossi2014metastream}, um \textit{framework} baseado em seleção periódica de algoritmos em fluxos de dados usando MtA, é aprimorado pela extensão de meta-atributos para mais modernos e informativos \cite{Rivolli2018}, e incluindo aprendizado incremental no nível meta, propondo o  LightGBM \cite{ke2017lightgbm} como meta-classificador, dado suas capacidades para lidar com esse problema específico. Seguimos então para investigar se meta-atributos de ponta e aprendizado incremental provido pelo LightGBM são capazes de predizer de forma mais precisa que o método base e aprimorar a performance geral do sistema aprendiz.