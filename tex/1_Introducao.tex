A iteração de usuários com sistemas digitais tem crescido exponencialmente com
o progresso tecnológico \cite{dean2008mapreduce,atzori2010internet}, sistemas
informáticos estão presentes no dia-a-dia das pessoas nas mais diversas formas,
\textit{smartphones} e seus diversos aplicativos, \textit{smart things} (TVs,
relógios, etc.), sensores, computadores de bordo e muitos outros dispositivos
são exemplos dessa tecnologia. Tais sistemas além estarem conectado a redes
internas e externas tem a capacidade de consumir e gerar um grande volume de
dados. Com isso uma quantidade massiva de dados se tornou disponível e esse
fenômeno ganhou um nome, ``\textit{Big Data}'' \cite{mcafee2012big}.

Essa grande quantidade de informação se tornou um dos bens mais valiosos do
mundo moderno \cite{economist2017world}, e na era do \textit{Big Data},
companhias como Google\regsymbol, Facebook\regsymbol, Netflix\regsymbol e
Amazon\regsymbol tem tirado proveito dessas grandes quantidades de dados
através da mineração com objetivo de extrair informações valiosas aos seus
usuários \cite{tarnoff2018big, finger2014data}, fenômeno só possível devido a
``injustificável eficácia dos dados'' \cite{halevy2009unreasonable}, em que
nesse volume, mesmo sem qualquer supervisão e filtragem no processo gerador dos
dados, essa quantidade massiva e complexa de dados, tornou viável muitas
tarefas de inteligência artificial de grande valor, como recomendações de
conteúdo (filmes, músicas, livros, etc.) ou filtrar conteúdos customizados,
passaram a apresentar um desempenho desejável.

Porém com novas possibilidades vieram também novos desafios, grande parte dos
sistemas computacionais que eram baseados no uso de memoria RAM para o
processamento se tornaram inviáveis para essa industria que produz dados na
escala de \textit{Petabytes} \cite{dean2008mapreduce}, logo, foi necessário a
concepção de novos ambientes para operarem no armazenamento e processamento
desses dados \cite{gama2007learning}. Ferramentas baseadas em computação
distribuída se apresentaram como uma alternativa viável devido seu potencial de
escalabilidade quase linear. Dentre essas ferramentas podemos exemplificar o
Apache Hadoop\textsuperscript{\tiny\textregistered}, que fornece um ambiente de
armazenamento distribuído, segmentando e replicando arquivos para que sejam
distribuídos aos nós de um \textit{cluster} de forma automática, o Apache
Spark\textsuperscript{\tiny\textregistered}, que provê um ambiente de
computação distribuída, seguindo o paradigma MapReduce e o Apache
Kafka\textsuperscript{\tiny\textregistered} o qual provê um ambiente para
operar fluxos de dados.

Cliques em paginas de mídias sociais, interações de usuários em serviços de
\textit{streaming}, monitoramento de ambientes por meio de sensores ou
atividades realizadas com uso de dispositivos \textit{smart}, são exemplos de
tecnologias que geram dados em um fluxo contínuo a uma alta frequência
\cite{gama2004learning}. O usual modelo colunar e estruturado não se apresentou
como uma forma eficaz de armazenar e manipular esse dados em tempo real
\cite{gama2007learning} e uma nova abordagem foi proposta. Fluxos de dados, ou
\textit{Data Streams}, são conjuntos de dados gerados em tempo real de forma
continua a frequências variáveis.



Uma das abordagens para extração de informação que tem sobressaído aliado ao
\textit{Big Data} é o Aprendizado de Máquina (AM). O AM é o campo da
inteligência artificial que estuda métodos para a extração de padrões dos
dados, para que esse conhecimento seja aplicado em tarefas futuras
\cite{mitchell1997machine,friedman2001elements}.

Como todo método indutivo, algoritmos tradicionais de AM assumem duas premissas
que devem ser satisfeitas para que funcionem corretamente: ($i$) o futuro deve
se comportar como o passado e ($ii$) eventos futuros devem ser independentes de
eventos passado \cite{vapnik2013nature}. Entretanto, em um fluxo de dados
continuo (em inglês, \textit{Data Stream}), a distribuição que gera os dados
usualmente está mudando com o tempo e dependências temporais podem ocorrer.
Logo, essas premissas podem não se manter e a performance dos modelos induzidos
podem perecer com o passar do tempo, causando uma má experiência aos usuários
desses sistemas  \cite{gama2007learning, Johansson2014}.

Diversos métodos foram desenvolvidos para lidar em ambientes com mudanças
temporais, como detecção de mudança de conceito
\cite{klinkenberg2000detecting}, retreino periódico de algoritmo
\cite{bifet2007learning} ou novas abordagens de algoritmos, especialmente
projetados para esse contexto \cite{zang2014comparative}.

Por exemplo, alguns métodos de detecção de mudança de conceito
\cite{gama2010knowledge} são baseados em alarmes que disparam quando a
diferença da performance do modelo atual e do melhor modelo até então é maior
que um dado limiar. Outros métodos, como a indução periódica de modelos, podem
não ser suficientes para resolver esses problemas, já que o espaço de hipótese
fixado do algoritmo pode não mais ser apropriado ao problema
\cite{rossi2014metastream}.

Meta-Aprendizado (MtA) é uma técnica proeminente para resolver essas limitações
através da detecção de mudança de conceito e recomendação de algoritmos para
melhorar a predição \cite{Anderson2019,VanRijn2016,Zarmehri2015}. Para usar
MtA, meta-dados devem ser construídos \cite{Vanschoren2018} baseado em uma
conjunto de características descritivas, nomeados meta-atributos, do problema
sob analise \cite{Rivolli2018}. Esses meta-atributos são extraídos e combinados
criando um meta-exemplo. Diferentes algoritmos de AM são aplicados ao problema,
e suas performances são utilizadas para rotular os meta-exemplos. Esse
procedimento é performado para o fluxo de dados em diferentes pontos no tempo,
resultando em um conjunto de meta-dados, o qual pode ser usado para induzir um
modelo capaz de predizer qual será o algoritmo mais apropriado para eventos
futuros sem os altos custos usuais que o processo de seleção de modelos de ML
demanda \cite{Munoz2018}.

Nesse trabalho o MetaStream \cite{rossi2012, rossi2014metastream}, um
\textit{framework} baseado em seleção periódica de algoritmos em fluxos de
dados usando MtA, é aprimorado pela extensão de meta-atributos para mais
modernos e informativos \cite{Rivolli2018}, e incluindo aprendizado incremental
no nível meta, propondo o  LightGBM \cite{ke2017lightgbm} como
meta-classificador, dado suas capacidades para lidar com esse problema
específico. Seguimos então para investigar se meta-atributos de ponta e
aprendizado incremental provido pelo LightGBM são capazes de predizer de forma
mais precisa que o método base e aprimorar a performance geral do sistema
aprendiz.


\section{Hipóteses}

\begin{enumerate}
    \item Em ambientes com mudança de conceito, viéses em um dado momento
        podem não ser adequados em momentos futuros.
    \item Algoritmos incrementais aprensentam um grau elevado de regularização,
        isso pode ser benéfico para alguns ambientes.
    \item Meta-Atributos modernos apresentam melhor poder descritivo,
        possibilitando uma melhora significativa em relação ao framework
        proposto originalmente.
\end{enumerate}

\section{Objetivos}

\begin{enumerate}
    \item Apresentar ganhos preditivos e em performance em relação ao framework
        original.
    \item Recomendar positivamente algoritmos de melhor performance ao longo do
        tempo.
\end{enumerate}
